# LLM Performance Benchmark Configuration
# (Local CPU-safe version)

benchmark:
  name: "llm_performance_benchmark"
  runs_per_prompt: 1
  seed: 42

# Model definitions (Hugging Face Hub)
# Using ONLY small model for local CPU testing
models:
  - id: "distilgpt2"
    name: "DistilGPT-2 (Small)"
    provider: "huggingface"
    size: "<1B"
    dtype: "float32"

# Dataset configuration
dataset:
  path: "config/prompts.jsonl"
  format: "jsonl"
  text_field: "prompt"
  max_prompts: 10

# Text generation parameters
generation:
  max_new_tokens: 128
  temperature: 0.7
  top_p: 0.9
  do_sample: true

# Runtime / device configuration
runtime:
  device: "cpu"           # cpu or cuda
  use_gpu_if_available: false
  batch_size: 1
  timeout_seconds: 60

# Output configuration
output:
  base_dir: "outputs"
  save_raw_outputs: true
  save_plots: true
  log_level: "INFO"
