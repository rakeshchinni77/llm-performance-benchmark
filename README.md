# LLM Performance Benchmarking Tool

A production-quality, configuration-driven **command-line benchmarking tool** to evaluate and compare the performance of Large Language Models (LLMs).

This project is built to help engineers and ML practitioners make **informed model selection decisions** by analyzing trade-offs between **latency, throughput, memory usage, and output quality** before deploying models to production.

---

## Project Objective

Selecting the right LLM for production is not only about output quality. Real-world systems must balance:

- Inference speed  
- Hardware resource usage  
- Scalability constraints  
- Cost and deployment feasibility  

This tool provides a **systematic and reproducible way** to benchmark multiple LLMs and generate **clear, actionable performance reports**.

---

## Key Features

- Command-line interface (CLI)
- Supports multiple Hugging Face LLMs
- Configuration-driven (YAML)
- Measures:
  - Inference latency
  - Tokens per second (throughput)
  - Peak RAM usage
  - Peak GPU memory usage (if available)
- Basic automated quality metrics:
  - Output length
  - Vocabulary diversity
- CSV result export
- Performance visualizations (PNG)
- Robust error handling (model failures donâ€™t crash runs)
- Fully tested with PyTest
- CI pipeline using GitHub Actions

---

## What This Tool Benchmarks

For each model and prompt, the tool records:

| Category | Metrics |
|--------|--------|
| Performance | Latency (seconds), Tokens/sec |
| Memory | Peak RAM, Peak GPU memory |
| Quality (basic) | Output length, Vocabulary diversity |

---

## Project Structure

```
llm-performance-benchmark/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ pyproject.toml
â”‚
â”œâ”€â”€ .github/
â”‚ â””â”€â”€ workflows/
â”‚ â””â”€â”€ ci.yml
â”‚
â”œâ”€â”€ config/
â”‚ â”œâ”€â”€ benchmark.yaml
â”‚ â”œâ”€â”€ prompts.jsonl
â”‚ â””â”€â”€ schema.yaml
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ cli.py
â”‚ â””â”€â”€ benchmark/
â”‚ â”œâ”€â”€ runner.py
â”‚ â”œâ”€â”€ models.py
â”‚ â”œâ”€â”€ metrics.py
â”‚ â”œâ”€â”€ monitor.py
â”‚ â”œâ”€â”€ reporter.py
â”‚ â”œâ”€â”€ environment.py
â”‚ â””â”€â”€ exceptions.py
â”‚
â”œâ”€â”€ tests/
â”‚ â”œâ”€â”€ test_cli.py
â”‚ â”œâ”€â”€ test_metrics.py
â”‚ â”œâ”€â”€ test_monitor.py
â”‚ â””â”€â”€ test_runner.py
â”‚
â”œâ”€â”€ outputs/
â”‚ â””â”€â”€ latest/
â”‚ â”œâ”€â”€ results.csv
â”‚ â”œâ”€â”€ latency_comparison.png
â”‚ â””â”€â”€ memory_comparison.png
â”‚
â””â”€â”€ docs/
â””â”€â”€ benchmark_report.md
```

---

## Setup Instructions

### 1ï¸.Clone the repository

```bash
git clone https://github.com/rakeshchinni77/llm-performance-benchmark.git
cd llm-performance-benchmark

```
### 2ï¸.Create and activate virtual environment
```bash
python -m venv .venv

```
Windows
```bash
.venv\Scripts\Activate.ps1
```
Linux / macOS
```bash
source .venv/bin/activate
```
### 3ï¸.Install dependencies
```bash
pip install -r requirements.txt
pip install -e .
```
---

## How to Run the Benchmark

The tool is executed via the CLI and controlled using a YAML configuration file.

```bash

python -m src.cli run --config config/benchmark.yaml

```
---

## Configuration Overview (benchmark.yaml)

The configuration file defines:

Models to benchmark

Dataset location

Generation parameters

Runtime device (CPU / GPU)

Output settings

Example (simplified):

```
models:
  - id: "distilgpt2"
    name: "DistilGPT-2 (Small)"
    provider: "huggingface"
    size: "<1B"
    dtype: "float32"

dataset:
  path: "config/prompts.jsonl"
  format: "jsonl"
  text_field: "prompt"

runtime:
  device: "cpu"

```
The configuration is validated using a JSON schema before execution.

---

## Output Artifacts

After execution, results are saved in:

```bash

outputs/latest/

```

---

## Visualizations

Latency Comparison
Helps identify fast vs slow models

Memory Usage Comparison
Highlights hardware feasibility

These charts allow quick comparison for production decision-making.

---

## Testing

All core components are unit tested.

Run tests locally:

```bash
pytest -v
```

---
## Continuous Integration (CI)

This repository includes a GitHub Actions workflow that:

Sets up Python 3.11

Installs dependencies

Runs all tests using PyTest

CI runs automatically on every push and pull request to main.

---

## Design Principles

1.Modularity â€“ clear separation of concerns

2.Configuration-driven â€“ no hardcoded models or paths

3.Fail-safe execution â€“ partial failures donâ€™t stop the run

4.Production mindset â€“ realistic metrics and reporting

5.Reproducibility â€“ environment metadata captured

---

### Benchmark Analysis

A detailed interpretation of the benchmark results is provided in:

ğŸ“˜ docs/benchmark_report.md

---

### Future Enhancements

Perplexity-based quality metrics

Batch inference benchmarking

Multi-GPU benchmarking

Cloud-hosted model evaluation

API endpoint benchmarking

---

### License

This project is released under the MIT License.


