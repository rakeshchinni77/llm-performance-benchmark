# LLM Performance Benchmarking Tool

A **production-quality, configuration-driven command-line benchmarking tool** for evaluating and comparing the performance of Large Language Models (LLMs).

This project helps engineers and ML practitioners make **informed model selection decisions** by systematically analyzing trade-offs between **latency, throughput, memory usage, and basic output quality** before deploying models to production.

---

## Project Objective

Selecting the right LLM for production is **not only about output quality**. Real-world systems must balance:

- Inference latency
- Hardware resource usage (CPU / GPU / RAM)
- Scalability constraints
- Deployment cost and feasibility

This tool provides a **systematic, reproducible, and automated way** to benchmark multiple LLMs and generate **clear, actionable performance reports**.

---

## Key Features

- Installable CLI tool (`llm-bench`)
- Supports Hugging Face LLMs
- Fully configuration-driven (YAML-based)
- Measures:
  - Inference latency
  - Tokens per second (throughput)
  - Peak RAM usage
  - Peak GPU memory usage (if available)
- Basic automated quality metrics:
  - Output length
  - Vocabulary diversity
- CSV result export
- Performance visualizations (PNG)
- Timestamped benchmark runs with `latest/` pointer
- Robust error handling (partial failures do not crash runs)
- Environment metadata capture (OS, Python version, hardware)
- Fully tested using PyTest
- CI pipeline via GitHub Actions

---

## What This Tool Benchmarks

For each model and prompt, the following metrics are recorded:

| Category        | Metrics                             |
| --------------- | ----------------------------------- |
| Performance     | Latency (seconds), Tokens/sec       |
| Memory          | Peak RAM (MB), Peak GPU memory (MB) |
| Quality (basic) | Output length, Vocabulary diversity |

---

## Project Structure

```
## Project Structure

llm-performance-benchmark/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ pyproject.toml
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ ci.yml
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ benchmark.yaml        # Model, dataset, and benchmark configuration
â”‚   â”œâ”€â”€ prompts.jsonl         # Benchmark prompts
â”‚   â””â”€â”€ schema.yaml           # Config validation schema
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cli.py                # CLI entry point
â”‚   â””â”€â”€ benchmark/
â”‚       â”œâ”€â”€ runner.py         # Benchmark orchestration logic
â”‚       â”œâ”€â”€ models.py         # Model loading and inference wrappers
â”‚       â”œâ”€â”€ metrics.py        # Performance and quality metrics
â”‚       â”œâ”€â”€ monitor.py        # RAM / GPU monitoring
â”‚       â”œâ”€â”€ reporter.py       # CSV + visualization generation
â”‚       â”œâ”€â”€ environment.py    # System metadata capture
â”‚       â”œâ”€â”€ logging_utils.py  # Structured logging utilities
â”‚       â””â”€â”€ exceptions.py     # Custom exception handling
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_cli.py
â”‚   â”œâ”€â”€ test_metrics.py
â”‚   â”œâ”€â”€ test_monitor.py
â”‚   â””â”€â”€ test_runner.py
â”‚
â”œâ”€â”€ outputs/                  # gitignored (runtime artifacts)
â”‚   â”œâ”€â”€ <timestamp>/
â”‚   â””â”€â”€ latest/
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ benchmark_report.md
    â”œâ”€â”€ design_decisions.md
    â”œâ”€â”€ sample_results.csv
    â”œâ”€â”€ latency_comparison.png
    â””â”€â”€ memory_comparison.png

```

---

## Setup Instructions

### 1ï¸.Clone the repository

```bash
git clone https://github.com/rakeshchinni77/llm-performance-benchmark.git
cd llm-performance-benchmark

```

### 2ï¸.Create and activate virtual environment

```bash
python -m venv .venv

```

Windows

```bash
.venv\Scripts\Activate.ps1
```

Linux / macOS

```bash
source .venv/bin/activate
```

### 3ï¸.Install dependencies

```bash
pip install -r requirements.txt
pip install -e .
```

---

## How to Run the Benchmark

The tool is executed via the CLI and controlled using a YAML configuration file.

```bash

python -m src.cli run --config config/benchmark.yaml

```

---

## Configuration Overview (`benchmark.yaml`)

The configuration file defines:

- Models to benchmark
- Prompt dataset
- Text generation parameters
- Runtime device (CPU / GPU)
- Output and logging behavior

### Example (simplified)

models:

- id: "distilgpt2"
  name: "DistilGPT-2 (Small)"
  provider: "huggingface"
  size: "<1B"
  dtype: "float32"

dataset:
path: "config/prompts.jsonl"
format: "jsonl"
text_field: "prompt"

runtime:
device: "cpu"

The configuration is validated against a **JSON schema** before execution to prevent runtime errors.

---

## Output Artifacts

Each benchmark run produces a timestamped directory:

outputs/<YYYY-MM-DD_HHMM>/

A convenience pointer is also maintained:

outputs/latest/

### Generated Artifacts

- results.csv
- latency_comparison.png
- memory_comparison.png
- environment.json
- summary.md
- logs/benchmark.log

**Note:** `outputs/` is gitignored.  
Sample artifacts are copied into `docs/` for review.

---

## Sample Visualizations

The following charts were generated from a real benchmark run and are included
in the `docs/` directory for reviewer inspection.

### ğŸ”¹ Average Inference Latency per Model
This chart compares the average end-to-end inference latency across models.

![Latency Comparison](docs/latency_comparison.png)

---

### ğŸ”¹ Peak Memory Usage per Model
This chart shows the average peak RAM usage during inference.

![Memory Usage Comparison](docs/memory_comparison.png)


---

## Testing

All major components are unit tested.

Run tests locally:

pytest -v

---

## Continuous Integration (CI)

A GitHub Actions workflow automatically:

- Sets up Python 3.11
- Installs dependencies
- Runs all PyTest test cases

CI executes on every push and pull request to `main`.

---

## Design Principles

- **Modularity** â€“ clear separation of concerns
- **Configuration-driven** â€“ no hardcoded models or paths
- **Fail-safe execution** â€“ partial failures do not stop the benchmark
- **Production mindset** â€“ realistic metrics and reporting
- **Reproducibility** â€“ environment metadata captured

Detailed rationale is available in:  
`docs/design_decisions.md`

---

## Benchmark Analysis

A detailed interpretation of the benchmark results is provided in:  
`docs/benchmark_report.md`

---

## Future Enhancements

- Perplexity-based quality metrics
- Batch inference benchmarking
- Multi-GPU benchmarking
- Cloud-hosted model evaluation
- API endpoint benchmarking

---

## License

This project is released under the **MIT License**.
